{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 'gulshan e iqbal')]\n",
      "[(2, 'defence')]\n",
      "[(1, 'clifton')]\n",
      "[(1, 'meher plaza')]\n",
      "[(1, 'al murtaza heights')]\n"
     ]
    }
   ],
   "source": [
    "correct_words = ['gulshan e hadeed', 'gulshan e iqbal', 'defence', 'clifton', 'meher plaza', 'al murtaza heights', 'al murtaza height']\n",
    "\n",
    "incorrect_words= ['gulshen iqbal', 'defnse', 'klifton', 'mehar plaza', 'al murteza heights']\n",
    "\n",
    "for word in incorrect_words:\n",
    "\t\ttemp = [(edit_distance(word, w),w) for w in correct_words]\n",
    "\t\t# print(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "\n",
    "\t\tif len(temp) > 1: \n",
    "\t\t\ttemp = [min(temp, key=lambda t: t[0])]\n",
    "\n",
    "\t\tprint(temp)\n",
    "\t\t# print(temp[0][1])\n",
    "\n",
    "# print(edit_distance(incorrect_words[2], correct_words[3]), correct_words[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main focus of address parsing - building names and uniformaty\n",
    "\n",
    "2 parts: \n",
    "- data parsing and storing in our DB (standart uniform way)\n",
    "- checking in DB, if same address fields are there so directly storing fields in db from prior knowledge\n",
    "\n",
    "data pipeline shape to the project (proper flow plus maximum automation (function automatic calling inside another function))\n",
    "\n",
    "organize files into folders, potiental folders (sub folders as required) : \n",
    "- code (main folder can already be code one)\n",
    "- data bases/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file 'abbreviations.json' has been modified.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def process_json(json_data):\n",
    "    processed_data = {}\n",
    "    for key, value in json_data.items():\n",
    "        processed_key = remove_punctuation(lowercase_text(key))\n",
    "        processed_value = remove_punctuation(lowercase_text(value))\n",
    "        processed_data[processed_key] = processed_value\n",
    "    return processed_data\n",
    "\n",
    "# Read the JSON file\n",
    "filename = \"abbreviations.json\"\n",
    "with open(filename) as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Process the JSON data\n",
    "processed_data = process_json(data)\n",
    "\n",
    "# Write the processed data back to the JSON file\n",
    "with open(filename, \"w\") as file:\n",
    "    json.dump(processed_data, file, indent=4)\n",
    "\n",
    "print(f\"JSON file '{filename}' has been modified.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "length of df = 213874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = ['block', 'phase', 'scheme', 'sector']\n",
    "\n",
    "street = ['street', 'phase', 'road', 'highway', 'khayaban']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Ticket #', 'Type' 'Unit #', 'Street Name', 'Building #', 'Building Name', 'City']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "place index number with every token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchy: \n",
    "\n",
    "(if present because not all feilds in all addresses)\n",
    "\n",
    "- Type --> INFERRED\n",
    "\n",
    "- Unit #\n",
    "\n",
    "\n",
    "- Building #\n",
    "\n",
    "- Building Name \n",
    "\n",
    "- Street Number/Name --> KEYWORD CHECK\n",
    "\n",
    "- Sub Area (phase, block, etc.) --> KEYWORD CHECK\n",
    "\n",
    "- Area (eg: bath island, civil lines) --> IF AVAILABLE IN END\n",
    "\n",
    "- Neighbourhood  --> STANDART, [-2] in address string\n",
    "\n",
    "- City --> STANDART, [-1] in address string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hierarchy step check:\n",
    "\n",
    "- pre_processing function --> grammer, punctuation, spacing, capitalization\n",
    "\n",
    "- tokenization\n",
    "\n",
    "- one by one tokenization feilds checks:\n",
    "\n",
    "    - spelling check (where applicable like neighbour hoods)\n",
    "\n",
    "    - checking if already in db\n",
    "\n",
    "    - keyword check where to place and place\n",
    "\n",
    "- place in DB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
