{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 'gulshan e iqbal')]\n",
      "[(2, 'defence')]\n",
      "[(1, 'clifton')]\n",
      "[(1, 'meher plaza')]\n",
      "[(1, 'al murtaza heights')]\n"
     ]
    }
   ],
   "source": [
    "correct_words = ['gulshan e hadeed', 'gulshan e iqbal', 'defence', 'clifton', 'meher plaza', 'al murtaza heights', 'al murtaza height']\n",
    "\n",
    "incorrect_words= ['gulshen iqbal', 'defnse', 'klifton', 'mehar plaza', 'al murteza heights']\n",
    "\n",
    "for word in incorrect_words:\n",
    "\t\ttemp = [(edit_distance(word, w),w) for w in correct_words]\n",
    "\t\t# print(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "\n",
    "\t\tif len(temp) > 1: \n",
    "\t\t\ttemp = [min(temp, key=lambda t: t[0])]\n",
    "\n",
    "\t\tprint(temp)\n",
    "\t\t# print(temp[0][1])\n",
    "\n",
    "# print(edit_distance(incorrect_words[2], correct_words[3]), correct_words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file 'abbreviations.json' has been modified.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def process_json(json_data):\n",
    "    processed_data = {}\n",
    "    for key, value in json_data.items():\n",
    "        processed_key = remove_punctuation(lowercase_text(key))\n",
    "        processed_value = remove_punctuation(lowercase_text(value))\n",
    "        processed_data[processed_key] = processed_value\n",
    "    return processed_data\n",
    "\n",
    "# Read the JSON file\n",
    "filename = \"abbreviations.json\"\n",
    "with open(filename) as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Process the JSON data\n",
    "processed_data = process_json(data)\n",
    "\n",
    "# Write the processed data back to the JSON file\n",
    "with open(filename, \"w\") as file:\n",
    "    json.dump(processed_data, file, indent=4)\n",
    "\n",
    "print(f\"JSON file '{filename}' has been modified.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_keywords = ['block', 'phase', 'scheme', 'sector']\n",
    "\n",
    "street_keywords = ['street', 'phase', 'road', 'highway', 'lane', 'avenue', 'boulevard', 'sharah']\n",
    "\n",
    "house_keywords = ['house', 'house no', 'house number', 'house #', 'plot']\n",
    "\n",
    "apartment_keywords = ['flat', 'flat no', 'flat number', 'flat #', 'apartment', 'building', 'suite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Ticket #', 'Type', 'House #', 'Appartment #', 'Building #', 'Building Name', 'Street Number/Name', 'Area & Sub Area', 'Neighbourhood', 'City']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# length of df = 213874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main focus of address parsing - building names and uniformaty\n",
    "\n",
    "2 parts: \n",
    "\n",
    "- data parsing and storing in our DB (standart uniform way)\n",
    "- checking in DB, if same address fields are there so directly storing fields in db from prior knowledge\n",
    "\n",
    "Work flows: \n",
    "- storing main address db in file, read and load from file, run parse (processing), update & add into main address db, store back in file\n",
    "- re structuring and removing unnecessary code\n",
    "- 2 major flow components for now: pre processing and how this (processing) where addressed is successfully parsed and placed into a df and saved\n",
    "\n",
    "data pipeline shape to the project (proper flow plus maximum automation (function automatic calling inside another function))\n",
    "\n",
    "- get number of unplaced entries in every address:\n",
    "\n",
    "    - len of tokenized list from parse before calling probabilistic_identifiers function\n",
    "\n",
    "    - total len of that tokenized address\n",
    "\n",
    "    - index scores with token of address \n",
    "    \n",
    "    - total average of 213874\n",
    "    \n",
    "    - data representation using mathplotlib of any tool\n",
    "\n",
    "\n",
    "- Seperate into 2 columns: Khayaban and Road, Street and Lane (differentiate the checks)\n",
    "\n",
    "\n",
    "-  Street is used in:\n",
    "\n",
    "    - DHA \n",
    "\n",
    "    - Nazimbad\n",
    "  \n",
    "- test data size should be 2% of original data size: \n",
    "    2% of 213874 = 4,277.48 = 4,275\n",
    "    So a test df of size 4,275 randomly generated \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchy: \n",
    "\n",
    "(if present because not all feilds in all addresses)\n",
    "\n",
    "- Type --> INFERRED\n",
    "\n",
    "- House # --> KEYWORD CHECK \n",
    "\n",
    "- Appartment # --> KEYWORD CHECK\n",
    "\n",
    "- Building # --> check with index +-1 if type int/number present\n",
    "\n",
    "- Building Name --> index +- >(len(tokenization list)/2)\n",
    "\n",
    "- Street Number/Name --> KEYWORD CHECK\n",
    "\n",
    "- Area (eg: bath island, civil lines, khayaban) --> IF AVAILABLE IN END & Sub Area (phase, block, etc.) --> KEYWORD CHECK\n",
    "\n",
    "- Neighbourhood  --> STANDART, [-2] in address string\n",
    "\n",
    "- City --> STANDART, [-1] in address string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hierarchy step check:\n",
    "\n",
    "- ~~pre_processing function --> grammer, punctuation, spacing, capitalization~~\n",
    "\n",
    "- ~~tokenization~~\n",
    "\n",
    "- one by one tokenization feilds checks:\n",
    "\n",
    "    - spelling check (where applicable like neighbour hoods)\n",
    "\n",
    "    - checking if already in db (or give if there exists another one with least Levenshtein distance, if Levenshtein distance < 2 then change to above one)\n",
    "\n",
    "    - keyword check where to place and place\n",
    "\n",
    "- place in DB\n",
    "\n",
    "- ~~when tokenization list empty, fill null or zero in remaining felids~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comma placement: \n",
    "\n",
    "check if commnas < 3 and street, house, appt, city, present in address string --> defense addreses (data2.csv)\n",
    "\n",
    "for commna placement: identifier checks --> check if 2 keywords present at once, if 2 keywords present at once, then break according to keyword i.e. place comma or break accordingly\n",
    "\n",
    "remaining address --> start 2 in building # & name, last 2 in area/sub area --> but first checks, if before len(tokenized address)/2 = half then for building if more than half than area (this will be checked for every area), buliding name not have int\n",
    "\n",
    "either ammend with keyword area approach or remove keyword area approach \n",
    "\n",
    "\n",
    "if type house and less than 50% present then areas and then fill none in remaining\n",
    "otherwise if type building then go for adding approach and add less than 50% ones to building # and name\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
