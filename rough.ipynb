{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 'gulshan e iqbal')]\n",
      "[(2, 'defence')]\n",
      "[(1, 'clifton')]\n",
      "[(1, 'meher plaza')]\n",
      "[(1, 'al murtaza heights')]\n"
     ]
    }
   ],
   "source": [
    "correct_words = ['gulshan e hadeed', 'gulshan e iqbal', 'defence', 'clifton', 'meher plaza', 'al murtaza heights', 'al murtaza height']\n",
    "\n",
    "incorrect_words= ['gulshen iqbal', 'defnse', 'klifton', 'mehar plaza', 'al murteza heights']\n",
    "\n",
    "for word in incorrect_words:\n",
    "\t\ttemp = [(edit_distance(word, w),w) for w in correct_words]\n",
    "\t\t# print(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "\n",
    "\t\tif len(temp) > 1: \n",
    "\t\t\ttemp = [min(temp, key=lambda t: t[0])]\n",
    "\n",
    "\t\tprint(temp)\n",
    "\t\t# print(temp[0][1])\n",
    "\n",
    "# print(edit_distance(incorrect_words[2], correct_words[3]), correct_words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Neighborhood\n",
      "0         abbas town\n",
      "1     abbasi shaheed\n",
      "2  abdul rehman goth\n",
      "3      abdullah goth\n",
      "4           abidabad\n",
      "File 'karachi_neighbourhoods_lowered.txt' has been created.\n"
     ]
    }
   ],
   "source": [
    "def load_corpus(file_name, pandas=False):\n",
    "    if pandas==True:\n",
    "        df = pd.read_csv(file_name, sep=\"\\t\", header=None, names=[\"Neighborhood\"])\n",
    "        return df\n",
    "    else:\n",
    "        with open(file_name, \"r\") as file:\n",
    "            neighborhood_names = file.readlines()    \n",
    "        refined_name_list = [name.strip() for name in neighborhood_names]\n",
    "        return refined_name_list\n",
    "\n",
    "\n",
    "def lowercase_conversion(text_str):\n",
    "    return text_str.lower()\n",
    "\n",
    "\n",
    "corpus_data = load_corpus(\"karachi_neighbourhoods.txt\")\n",
    "\n",
    "file_name = \"karachi_neighbourhoods_lowered.txt\"\n",
    "\n",
    "with open(file_name, \"w\") as file:\n",
    "    for item in corpus_data:\n",
    "        item = lowercase_conversion(item)\n",
    "        file.write(item + \"\\n\")\n",
    "\n",
    "new_corpus = load_corpus(\"karachi_neighbourhoods_lowered.txt\", True)\n",
    "\n",
    "print(new_corpus.head())\n",
    "\n",
    "print(f\"File '{file_name}' has been created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    'Apt': 'Apartment',\n",
    "    'Bldg.': 'Building',\n",
    "    'Cantt.': 'Cantonment',\n",
    "    'Col.': 'Colony',\n",
    "    'Ext.': 'Extension',\n",
    "    'Hno.': 'House Number',\n",
    "    'Mkt.': 'Market',\n",
    "    'Ngr.': 'Nagar',\n",
    "    'P.O.': 'Post Office',\n",
    "    'P.O.B': 'Post Office Box',\n",
    "    'Pl.': 'Place',\n",
    "    'Sec.': 'Sector',\n",
    "    'Twn.': 'Town',\n",
    "    'Vlg.': 'Village',\n",
    "    'Dist.': 'District',\n",
    "    'Rd.': 'Road',\n",
    "    'Jct.': 'Junction',\n",
    "    'Ln.': 'Lane',\n",
    "    'Masjid': 'Mosque',\n",
    "    'Fl.': 'Floor',\n",
    "    'Blk.': 'Block',\n",
    "    'Sch.': 'School',\n",
    "    'Univ.': 'University',\n",
    "    'Corp.': 'Corporation',\n",
    "    'Ctr.': 'Center',\n",
    "    'Plz.': 'Plaza',\n",
    "    'Ph.': 'Phase',\n",
    "    'G.P.O.': 'General Post Office',\n",
    "    'Bzr.': 'Bazaar',\n",
    "    'Est.': 'Estate',\n",
    "    'St.': 'Street',\n",
    "    'Ave': 'Avenue',\n",
    "    'Blvd': 'Boulevard',\n",
    "    'Pkwy': 'Parkway',\n",
    "    'Hwy': 'Highway',\n",
    "    'Res.': 'Residential',\n",
    "    'Phase': 'Phase',\n",
    "    'Khy':'Khayaban'\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "punctuation removing: (from sources)\n",
    "- city areas corpus (only has dashes -) -> (solved for capitalization)\n",
    "- abbriviations json -> punctuation + capitalization\n",
    "- data to process -> lowered plus removing all capitalization except for , and - \n",
    "\n",
    "main focus of address parsing - building names and uniformaty\n",
    "\n",
    "2 parts: \n",
    "- data parsing and storing in our DB (standart uniform way)\n",
    "- checking in DB, if same address fields are there so directly storing fields in db from prior knowledge\n",
    "\n",
    "data pipeline shape to the project (proper flow plus maximum automation (function automatic calling inside another function))\n",
    "\n",
    "organize files into folders, potiental folders (sub folders as required) : \n",
    "- code (main folder can already be code one)\n",
    "- data bases/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = 'House # C38, Block 8, Gulshan-e-Iqbal, Karachi'\n",
    "sample2 = 'House No. 123, Street 5, Phase 7, DHA'\n",
    "sample3 = \"House No. 123, St. 5, Phase 7, DHA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_address(address):\n",
    "    # Word Tokenization\n",
    "    tokens = address.split()\n",
    "\n",
    "    # Regex Tokenization for capturing specific patterns\n",
    "    # You can customize the regex patterns based on the address format in Pakistan\n",
    "    regex_patterns = [\n",
    "        r'\\d+',  # Capture numerical values (e.g., house numbers)\n",
    "        r'[A-Za-z]+',  # Capture alphabetic words (e.g., street names, city names)\n",
    "        r'\\b[A-Za-z]{2}\\b'  # Capture two-letter abbreviations (e.g., state codes)\n",
    "    ]\n",
    "    tokens_new = []\n",
    "    for pattern in regex_patterns:\n",
    "        regex_tokens = re.findall(pattern, address)\n",
    "        tokens_new.extend(regex_tokens)\n",
    "\n",
    "    return tokens_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', 'Main', 'Street', 'Lahore', 'Punjab']\n",
      "['38', '8', 'House', 'C', 'Block', 'Gulshan', 'e', 'Iqbal', 'Karachi']\n",
      "['123', '5', '7', 'House', 'No', 'Street', 'Phase', 'DHA', 'No']\n",
      "['123', '5', '7', 'House', 'No', 'St', 'Phase', 'DHA', 'No', 'St']\n"
     ]
    }
   ],
   "source": [
    "address = \"12, Main Street, Lahore, Punjab\"\n",
    "# tokens = tokenize_address(address)\n",
    "# print(tokens)\n",
    "\n",
    "print(tokenize_address(address))\n",
    "print(tokenize_address(sample1))\n",
    "print(tokenize_address(sample2))\n",
    "print(tokenize_address(sample3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address 1 Type: House\n",
      "Address 2 Type: Apartment\n",
      "Address 3 Type: Unknown\n",
      "Address 4 Type: Apartment\n",
      "Address 5 Type: Both House and Apartment\n"
     ]
    }
   ],
   "source": [
    "def check_address_type(address):\n",
    "    address = address.lower()\n",
    "\n",
    "    house_keywords = ['house no', 'house number', 'house #', 'house']\n",
    "    apartment_keywords = ['flat no', 'flat number', 'flat #', 'flat', 'apartment', 'building']\n",
    "\n",
    "    house_found = any(keyword in address for keyword in house_keywords)\n",
    "    apartment_found = any(keyword in address for keyword in apartment_keywords)\n",
    "\n",
    "    if house_found and apartment_found:\n",
    "        return 'Both House and Apartment'\n",
    "    elif house_found:\n",
    "        return 'House'\n",
    "    elif apartment_found:\n",
    "        return 'Apartment'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Example usage:\n",
    "address1 = \"House No. 123, Main Street, New York\"\n",
    "address2 = \"Apartment 7B, Building X, Chicago\"\n",
    "address3 = \"1234 Park Avenue, Los Angeles\"\n",
    "address4 = \"Flat No. 456, ABC Apartments, London\"\n",
    "address5 = \"House #789, XYZ Building, Sydney\"\n",
    "\n",
    "type1 = check_address_type(address1)\n",
    "type2 = check_address_type(address2)\n",
    "type3 = check_address_type(address3)\n",
    "type4 = check_address_type(address4)\n",
    "type5 = check_address_type(address5)\n",
    "\n",
    "print(f\"Address 1 Type: {type1}\")\n",
    "print(f\"Address 2 Type: {type2}\")\n",
    "print(f\"Address 3 Type: {type3}\")\n",
    "print(f\"Address 4 Type: {type4}\")\n",
    "print(f\"Address 5 Type: {type5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file 'abbreviations.json' has been modified.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def process_json(json_data):\n",
    "    processed_data = {}\n",
    "    for key, value in json_data.items():\n",
    "        processed_key = remove_punctuation(lowercase_text(key))\n",
    "        processed_value = remove_punctuation(lowercase_text(value))\n",
    "        processed_data[processed_key] = processed_value\n",
    "    return processed_data\n",
    "\n",
    "# Read the JSON file\n",
    "filename = \"abbreviations.json\"\n",
    "with open(filename) as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Process the JSON data\n",
    "processed_data = process_json(data)\n",
    "\n",
    "# Write the processed data back to the JSON file\n",
    "with open(filename, \"w\") as file:\n",
    "    json.dump(processed_data, file, indent=4)\n",
    "\n",
    "print(f\"JSON file '{filename}' has been modified.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
