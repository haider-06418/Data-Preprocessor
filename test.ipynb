{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Address2Building: Deep Learning-Based Building Name Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other names:\n",
    "\n",
    "1. **BERT-Based Building Name Extractor (BBNE)**\n",
    "2. **Address2Building: Deep Learning-Based Building Name Extraction**\n",
    "3. **BERTex: BERT Enhanced Text Extractor for Building Names**\n",
    "4. **BuildBERT: Address Parsing and Building Name Recognition**\n",
    "5. **AddressNet: Building Name Extraction Using BERT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import data_preprocessor\n",
    "from config import *\n",
    "import string\n",
    "\n",
    "abbreviations = data_preprocessor.load_json(\"abbreviations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71318\n"
     ]
    }
   ],
   "source": [
    "df = data_preprocessor.load_corpus(fname, pandas = True, header = True)\n",
    "df = df.drop(columns=['Title', 'Created', 'Close Time', 'Queue'], axis=1) \n",
    "\n",
    "lst_addresses = df['Address'].tolist()\n",
    "\n",
    "print(len(lst_addresses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_address(address):\n",
    "    address = address.lower()  # Convert to lowercase\n",
    "    address = address.strip()  # Remove leading and trailing whitespaces\n",
    "    address = ' '.join(address.split())  # Replace multiple spaces with a single space\n",
    "    address.translate(str.maketrans('', '', string.punctuation)) # Removing punctuation\n",
    "    address = data_preprocessor.standard_abbreviations_fix(address, abbreviations) # Standardizing Abbreviations\n",
    "    # You can add more cleaning steps if necessary\n",
    "    return address\n",
    "\n",
    "def truncate_address(address, max_length=512):\n",
    "    tokens = address.split()\n",
    "    if len(tokens) > max_length:\n",
    "        return ' '.join(tokens[:max_length])\n",
    "    return address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(addresses, labels):\n",
    "    cleaned_addresses = [clean_address(a) for a in addresses]\n",
    "    truncated_addresses = [truncate_address(a) for a in cleaned_addresses]\n",
    "    tokenized_addresses = [tokenize_for_bert(a) for a in truncated_addresses]\n",
    "    spans = [convert_labels_to_spans(a, l, tokenizer) for a, l in zip(truncated_addresses, labels)]\n",
    "    \n",
    "    return tokenized_addresses, spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "addresses = [\"your_list_of_addresses_here\"]\n",
    "labels = [\"your_list_of_building_name_spans_here\"]\n",
    "\n",
    "# Split into train and test sets (80-20)\n",
    "addresses_train, addresses_test, labels_train, labels_test = train_test_split(addresses, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training set into training and validation (80-20)\n",
    "addresses_train, addresses_val, labels_train, labels_val = train_test_split(addresses_train, labels_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_for_bert(address, max_length=512):\n",
    "    return tokenizer.encode_plus(address, \n",
    "                                 add_special_tokens=True,\n",
    "                                 max_length=max_length,\n",
    "                                 pad_to_max_length=True,\n",
    "                                 return_attention_mask=True)\n",
    "\n",
    "\n",
    "def convert_labels_to_spans(address, label, tokenizer, max_length=512):\n",
    "    # Tokenize the address and the label\n",
    "    tokens = tokenizer.tokenize(address)\n",
    "    label_tokens = tokenizer.tokenize(label)\n",
    "    \n",
    "    # Find the start and end token positions of the label in the address tokens\n",
    "    try:\n",
    "        start_idx = tokens.index(label_tokens[0])\n",
    "        end_idx = start_idx + len(label_tokens) - 1\n",
    "    except ValueError:\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 5), (5, 6), (5, 6)]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def find_closest_span(address_tokens, building_name_tokens):\n",
    "    len_name = len(building_name_tokens)\n",
    "    best_match = (0, 0)\n",
    "    best_score = -1\n",
    "\n",
    "    for i in range(len(address_tokens) - len_name + 1):\n",
    "        score = sum([1 if address_tokens[i + j] == building_name_tokens[j] else 0 for j in range(len_name)])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = (i, i + len_name - 1)\n",
    "\n",
    "    return best_match\n",
    "\n",
    "def generate_spans(addresses, building_names, tokenizer):\n",
    "    spans = []\n",
    "\n",
    "    for address, building_name in zip(addresses, building_names):\n",
    "        address_tokens = tokenizer.tokenize(address)\n",
    "        building_name_tokens = tokenizer.tokenize(building_name)\n",
    "\n",
    "        span = find_closest_span(address_tokens, building_name_tokens)\n",
    "        spans.append(span)\n",
    "\n",
    "    return spans\n",
    "\n",
    "# Sample data\n",
    "addresses = [\"1234 Elm St WindsorBuilding Apt 4B\", \"5678 Oak Rd MapleComplex Level 2\", \"4321 Pine Ln Evergreen Estate Block B\"]\n",
    "building_names = [\"Windsor Building\", \"Maple Complex\", \"Evergreen Estate\"]\n",
    "\n",
    "# Generate spans\n",
    "spans = generate_spans(addresses, building_names, tokenizer)\n",
    "print(spans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW, BertConfig, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Custom dataset class\n",
    "class AddressDataset(Dataset):\n",
    "    def __init__(self, tokenizer, addresses, spans):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.addresses = addresses\n",
    "        self.spans = spans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.addresses)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        address = self.addresses[idx]\n",
    "        start, end = self.spans[idx]\n",
    "\n",
    "        encoding = self.tokenizer(address, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        encoding['start_positions'] = torch.tensor(start)\n",
    "        encoding['end_positions'] = torch.tensor(end)\n",
    "\n",
    "        return encoding\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample data (you would replace this with your actual data)\n",
    "addresses = [\"1234 Elm St Windsor Building\", \"5678 Oak Rd Maple Complex\"]\n",
    "spans = [(4, 6), (4, 6)]  # This is just dummy data. The actual spans would point to the building names' start and end token positions.\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = AddressDataset(tokenizer, addresses, spans)\n",
    "dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# Define training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# After training, the model can be used to predict the start and end tokens of building names in new addresses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 1 - Google BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import data_preprocessor\n",
    "import string\n",
    "\n",
    "abbreviations = data_preprocessor.load_json(\"abbreviations.json\")\n",
    "fname = 'Data/buildings/Buildings_Dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3550\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>Building Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House # B6, Block-B Floor Aftab Sultan residen...</td>\n",
       "      <td>Aftab Sultan Complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apartment/Suite# B-3 , Building Block B, Aftab...</td>\n",
       "      <td>Aftab Sultan Complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apartment/Suite# B-1 1st Floor, Building Block...</td>\n",
       "      <td>Aftab Sultan Complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>House # Aftab Sultan Resedention complex Appt ...</td>\n",
       "      <td>Aftab Sultan Complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>House # St 20 fL B2 Aftab sultan near postoffi...</td>\n",
       "      <td>Aftab Sultan Complex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Address         Building Name\n",
       "0  House # B6, Block-B Floor Aftab Sultan residen...  Aftab Sultan Complex\n",
       "1  Apartment/Suite# B-3 , Building Block B, Aftab...  Aftab Sultan Complex\n",
       "2  Apartment/Suite# B-1 1st Floor, Building Block...  Aftab Sultan Complex\n",
       "3  House # Aftab Sultan Resedention complex Appt ...  Aftab Sultan Complex\n",
       "4  House # St 20 fL B2 Aftab sultan near postoffi...  Aftab Sultan Complex"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_preprocessor.load_corpus(fname, pandas = True, header = True)\n",
    "# df = df.drop(columns=['Title', 'Created', 'Close Time', 'Queue'], axis=1) \n",
    "\n",
    "addresses = df['Address'].tolist()\n",
    "building_names = df['Building Name'].tolist()\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702\n"
     ]
    }
   ],
   "source": [
    "# building_names = data_preprocessor.load_corpus('karachi_buildings.txt')\n",
    "print(len(building_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data without commas; replace with your actual dataset\n",
    "addresses = [\n",
    "    \"123 Elm St Windsor Building Apt 5A\",\n",
    "    \"456 Oak Rd Maple Complex Level 2\",\n",
    "    \"789 Pine Ave Cedar Towers Block B\"\n",
    "]\n",
    "\n",
    "building_names = [\n",
    "    \"Windsor Building\",\n",
    "    \"Maple Complex\",\n",
    "    \"Cedar Towers\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "addresses_train, addresses_test, building_names_train, building_names_test = train_test_split(addresses, building_names, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into 80% training and 20% validation\n",
    "addresses_train, addresses_val, building_names_train, building_names_val = train_test_split(addresses_train, building_names_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Haider.Abbad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initializing BERT Model\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_address(address):\n",
    "    address = address.lower()  # Convert to lowercase\n",
    "    address = address.strip()  # Remove leading and trailing whitespaces\n",
    "    address = ' '.join(address.split())  # Replace multiple spaces with a single space\n",
    "    # address.translate(str.maketrans('', '', string.punctuation)) # Removing punctuation\n",
    "    # address = data_preprocessor.standard_abbreviations_fix(address, abbreviations) # Standardizing Abbreviations\n",
    "    # You can add more cleaning steps if necessary\n",
    "    return address\n",
    "\n",
    "\n",
    "def tokenize_for_bert(address, max_length=512):\n",
    "    return tokenizer.encode_plus(address, \n",
    "                                 add_special_tokens=True,\n",
    "                                 max_length=max_length,\n",
    "                                 pad_to_max_length=True,\n",
    "                                 return_attention_mask=True,\n",
    "                                 truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "def convert_labels_to_spans(address, building_name, tokenizer, max_length=512):\n",
    "    # Tokenize the address and the building name\n",
    "    address_tokens = tokenizer.tokenize(address)\n",
    "    building_name_tokens = tokenizer.tokenize(building_name)\n",
    "    \n",
    "    # Find the start and end token positions of the building name in the address tokens\n",
    "    try:\n",
    "        start_idx = address_tokens.index(building_name_tokens[0])\n",
    "        end_idx = start_idx + len(building_name_tokens) - 1\n",
    "    except ValueError:\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(addresses, building_names):\n",
    "    cleaned_addresses = [clean_address(a) for a in addresses]\n",
    "    tokenized_data = [tokenize_for_bert(a) for a in cleaned_addresses]\n",
    "    input_ids = [item['input_ids'] for item in tokenized_data]\n",
    "    attention_masks = [item['attention_mask'] for item in tokenized_data]\n",
    "    \n",
    "    spans = [convert_labels_to_spans(a, b, tokenizer) for a, b in zip(cleaned_addresses, building_names)]\n",
    "    start_positions = [span[0] for span in spans]\n",
    "    end_positions = [span[1] for span in spans]\n",
    "    \n",
    "    return input_ids, attention_masks, start_positions, end_positions\n",
    "\n",
    "\n",
    "input_ids_train, attention_masks_train, start_positions_train, end_positions_train = preprocess_data(addresses_train, building_names_train)\n",
    "input_ids_val, attention_masks_val, start_positions_val, end_positions_val = preprocess_data(addresses_val, building_names_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "# Convert data into torch tensors\n",
    "input_ids_train = torch.tensor(input_ids_train)\n",
    "attention_masks_train = torch.tensor(attention_masks_train)\n",
    "start_positions_train = torch.tensor(start_positions_train)\n",
    "end_positions_train = torch.tensor(end_positions_train)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_data = TensorDataset(input_ids_train, attention_masks_train, start_positions_train, end_positions_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)  # You can adjust batch size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Haider.Abbad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 3)  # Assuming 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # you can adjust the number of epochs\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Load batch data to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_attention_mask, b_start_positions, b_end_positions = batch\n",
    "\n",
    "        # Clear any previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, start_positions=b_start_positions, end_positions=b_end_positions)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping (optional, can help prevent exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss over the training data\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "\n",
    "\"\"\" Do note: Training a BERT model can be resource-intensive. Ideally, this should be run on a machine with a good GPU. Adjust \n",
    "the batch size and learning rate according to the resources available and monitor for any potential issues during training.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haider.Abbad\\AppData\\Local\\Temp\\ipykernel_13928\\1678758524.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_val = torch.tensor(input_ids_val)\n",
      "C:\\Users\\Haider.Abbad\\AppData\\Local\\Temp\\ipykernel_13928\\1678758524.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_masks_val = torch.tensor(attention_masks_val)\n",
      "C:\\Users\\Haider.Abbad\\AppData\\Local\\Temp\\ipykernel_13928\\1678758524.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  start_positions_val = torch.tensor(start_positions_val)\n",
      "C:\\Users\\Haider.Abbad\\AppData\\Local\\Temp\\ipykernel_13928\\1678758524.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  end_positions_val = torch.tensor(end_positions_val)\n"
     ]
    }
   ],
   "source": [
    "# Convert validation data into torch tensors\n",
    "input_ids_val = torch.tensor(input_ids_val)\n",
    "attention_masks_val = torch.tensor(attention_masks_val)\n",
    "start_positions_val = torch.tensor(start_positions_val)\n",
    "end_positions_val = torch.tensor(end_positions_val)\n",
    "\n",
    "# Create a DataLoader for validation data\n",
    "val_data = TensorDataset(input_ids_val, attention_masks_val, start_positions_val, end_positions_val)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=16)  # Adjust batch size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.86\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_eval_loss = 0\n",
    "\n",
    "all_start_positions = []\n",
    "all_end_positions = []\n",
    "all_pred_start_positions = []\n",
    "all_pred_end_positions = []\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    # Load batch data to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_attention_mask, b_start_positions, b_end_positions = batch\n",
    "\n",
    "    # Tell the model not to compute gradients\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, start_positions=b_start_positions, end_positions=b_end_positions)\n",
    "        \n",
    "    # Get the predicted start and end token positions\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    pred_start_positions = torch.argmax(start_logits, dim=1)\n",
    "    pred_end_positions = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "    loss = outputs[0]\n",
    "    total_eval_loss += loss.item()\n",
    "\n",
    "    all_start_positions.extend(b_start_positions.tolist())\n",
    "    all_end_positions.extend(b_end_positions.tolist())\n",
    "    all_pred_start_positions.extend(pred_start_positions.tolist())\n",
    "    all_pred_end_positions.extend(pred_end_positions.tolist())\n",
    "\n",
    "avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "print(f\"Validation Loss: {avg_eval_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) Score on Validation Set: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def compute_exact_match(true_starts, true_ends, pred_starts, pred_ends):\n",
    "    return sum([(ts == ps) and (te == pe) for ts, te, ps, pe in zip(true_starts, true_ends, pred_starts, pred_ends)])\n",
    "\n",
    "EM_score = compute_exact_match(all_start_positions, all_end_positions, all_pred_start_positions, all_pred_end_positions)\n",
    "print(f\"Exact Match (EM) Score on Validation Set: {EM_score / len(all_start_positions):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Deployment & Usage (Simplified for Direct Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths where the model and tokenizer were saved\n",
    "model_save_path = './model_save/'\n",
    "tokenizer_save_path = './tokenizer_save/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer_save/tokenizer_config.json',\n",
       " './tokenizer_save/special_tokens_map.json',\n",
       " './tokenizer_save/vocab.txt',\n",
       " './tokenizer_save/added_tokens.json')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(model_save_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_save_path)\n",
    "\n",
    "# If you have a GPU, let's put the model there for faster computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_building_name_from_span(address, start_idx, end_idx, tokenizer):\n",
    "#     tokens = tokenizer.tokenize(address)\n",
    "#     building_name_tokens = tokens[start_idx: end_idx+1]\n",
    "#     building_name = tokenizer.convert_tokens_to_string(building_name_tokens)\n",
    "#     return building_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_building_names(addresses, model, tokenizer):\n",
    "    # Preprocess the input addresses\n",
    "    input_ids = [tokenizer.encode(a, add_special_tokens=True, max_length=512, pad_to_max_length=True) for a in addresses]\n",
    "    attention_masks = [[1 if token_id > 0 else 0 for token_id in address] for address in input_ids]\n",
    "    \n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    attention_masks = torch.tensor(attention_masks).to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "    start_logits = outputs.start_logits.cpu().numpy()\n",
    "    end_logits = outputs.end_logits.cpu().numpy()\n",
    "\n",
    "    predicted_start = start_logits.argmax(axis=1)\n",
    "    predicted_end = end_logits.argmax(axis=1)\n",
    "\n",
    "    # Post-process to extract building names\n",
    "    building_names = []\n",
    "    for i, address in enumerate(addresses):\n",
    "        tokens = tokenizer.tokenize(address)\n",
    "        building_name_tokens = tokens[predicted_start[i]:predicted_end[i]+1]\n",
    "        building_name = tokenizer.convert_tokens_to_string(building_name_tokens)\n",
    "        building_names.append(building_name)\n",
    "\n",
    "    return building_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['windsor building apt 5a', 'maple complex', 'cedar towers block b']\n"
     ]
    }
   ],
   "source": [
    "# Sample usage:\n",
    "addresses_list = [\n",
    "    \"123 Elm St Windsor Building Apt 5A\",\n",
    "    \"456 Oak Rd Maple Complex Level 2\",\n",
    "    \"789 Pine Ave Cedar Towers Block B\"\n",
    "]\n",
    "\n",
    "predicted_building_names = extract_building_names(addresses_list, model, tokenizer)\n",
    "print(predicted_building_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
