{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Address2Building: Deep Learning-Based Building Name Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other names:\n",
    "\n",
    "1. **BERT-Based Building Name Extractor (BBNE)**\n",
    "2. **Address2Building: Deep Learning-Based Building Name Extraction**\n",
    "3. **BERTex: BERT Enhanced Text Extractor for Building Names**\n",
    "4. **BuildBERT: Address Parsing and Building Name Recognition**\n",
    "5. **AddressNet: Building Name Extraction Using BERT**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import data_preprocessor\n",
    "import string\n",
    "\n",
    "abbreviations = data_preprocessor.load_json(\"abbreviations.json\")\n",
    "fname = 'Data/buildings/Buildings_Dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3550\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>Building Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House # B6, Block-B Floor Aftab Sultan residen...</td>\n",
       "      <td>Aftab Sultan Complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apartment/Suite# B-3 , Building Block B, Aftab...</td>\n",
       "      <td>Aftab Sultan Complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apartment/Suite# B-1 1st Floor, Building Block...</td>\n",
       "      <td>Aftab Sultan Complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>House # Aftab Sultan Resedention complex Appt ...</td>\n",
       "      <td>Aftab Sultan Complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>House # St 20 fL B2 Aftab sultan near postoffi...</td>\n",
       "      <td>Aftab Sultan Complex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Address         Building Name\n",
       "0  House # B6, Block-B Floor Aftab Sultan residen...  Aftab Sultan Complex\n",
       "1  Apartment/Suite# B-3 , Building Block B, Aftab...  Aftab Sultan Complex\n",
       "2  Apartment/Suite# B-1 1st Floor, Building Block...  Aftab Sultan Complex\n",
       "3  House # Aftab Sultan Resedention complex Appt ...  Aftab Sultan Complex\n",
       "4  House # St 20 fL B2 Aftab sultan near postoffi...  Aftab Sultan Complex"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data_preprocessor.load_corpus(fname, pandas = True, header = True)\n",
    "# df = df.drop(columns=['Title', 'Created', 'Close Time', 'Queue'], axis=1) \n",
    "\n",
    "addresses = df['Address'].tolist()\n",
    "building_names = df['Building Name'].tolist()\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building_names = data_preprocessor.load_corpus('karachi_buildings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample data without commas; replace with your actual dataset\n",
    "# addresses = [\n",
    "#     \"123 Elm St Windsor Building Apt 5A\",\n",
    "#     \"456 Oak Rd Maple Complex Level 2\",\n",
    "#     \"789 Pine Ave Cedar Towers Block B\"\n",
    "# ]\n",
    "\n",
    "# building_names = [\n",
    "#     \"Windsor Building\",\n",
    "#     \"Maple Complex\",\n",
    "#     \"Cedar Towers\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "addresses_train, addresses_test, building_names_train, building_names_test = train_test_split(addresses, building_names, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into 80% training and 20% validation\n",
    "addresses_train, addresses_val, building_names_train, building_names_val = train_test_split(addresses_train, building_names_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Haider.Abbad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initializing BERT Model\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_address(address):\n",
    "    address = address.lower()  # Convert to lowercase\n",
    "    address = address.strip()  # Remove leading and trailing whitespaces\n",
    "    address = ' '.join(address.split())  # Replace multiple spaces with a single space\n",
    "    # address.translate(str.maketrans('', '', string.punctuation)) # Removing punctuation\n",
    "    # address = data_preprocessor.standard_abbreviations_fix(address, abbreviations) # Standardizing Abbreviations\n",
    "    # You can add more cleaning steps if necessary\n",
    "    return address\n",
    "\n",
    "\n",
    "def tokenize_for_bert(address, max_length=512):\n",
    "    return tokenizer.encode_plus(address, \n",
    "                                 add_special_tokens=True,\n",
    "                                 max_length=max_length,\n",
    "                                 pad_to_max_length=True,\n",
    "                                 return_attention_mask=True,\n",
    "                                 truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "def convert_labels_to_spans(address, building_name, tokenizer, max_length=512):\n",
    "    # Tokenize the address and the building name\n",
    "    address_tokens = tokenizer.tokenize(address)\n",
    "    building_name_tokens = tokenizer.tokenize(building_name)\n",
    "    \n",
    "    # Find the start and end token positions of the building name in the address tokens\n",
    "    try:\n",
    "        start_idx = address_tokens.index(building_name_tokens[0])\n",
    "        end_idx = start_idx + len(building_name_tokens) - 1\n",
    "    except ValueError:\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Haider.Abbad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(addresses, building_names):\n",
    "    cleaned_addresses = [clean_address(a) for a in addresses]\n",
    "    tokenized_data = [tokenize_for_bert(a) for a in cleaned_addresses]\n",
    "    input_ids = [item['input_ids'] for item in tokenized_data]\n",
    "    attention_masks = [item['attention_mask'] for item in tokenized_data]\n",
    "    \n",
    "    spans = [convert_labels_to_spans(a, b, tokenizer) for a, b in zip(cleaned_addresses, building_names)]\n",
    "    start_positions = [span[0] for span in spans]\n",
    "    end_positions = [span[1] for span in spans]\n",
    "    \n",
    "    return input_ids, attention_masks, start_positions, end_positions\n",
    "\n",
    "\n",
    "input_ids_train, attention_masks_train, start_positions_train, end_positions_train = preprocess_data(addresses_train, building_names_train)\n",
    "input_ids_val, attention_masks_val, start_positions_val, end_positions_val = preprocess_data(addresses_val, building_names_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "# Convert data into torch tensors\n",
    "input_ids_train = torch.tensor(input_ids_train)\n",
    "attention_masks_train = torch.tensor(attention_masks_train)\n",
    "start_positions_train = torch.tensor(start_positions_train)\n",
    "end_positions_train = torch.tensor(end_positions_train)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_data = TensorDataset(input_ids_train, attention_masks_train, start_positions_train, end_positions_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)  # You can adjust batch size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Haider.Abbad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 3)  # Assuming 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # you can adjust the number of epochs\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Load batch data to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_attention_mask, b_start_positions, b_end_positions = batch\n",
    "\n",
    "        # Clear any previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, start_positions=b_start_positions, end_positions=b_end_positions)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping (optional, can help prevent exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss over the training data\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "\n",
    "\"\"\" Do note: Training a BERT model can be resource-intensive. Ideally, this should be run on a machine with a good GPU. Adjust \n",
    "the batch size and learning rate according to the resources available and monitor for any potential issues during training.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haider.Abbad\\AppData\\Local\\Temp\\ipykernel_13928\\1678758524.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_val = torch.tensor(input_ids_val)\n",
      "C:\\Users\\Haider.Abbad\\AppData\\Local\\Temp\\ipykernel_13928\\1678758524.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_masks_val = torch.tensor(attention_masks_val)\n",
      "C:\\Users\\Haider.Abbad\\AppData\\Local\\Temp\\ipykernel_13928\\1678758524.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  start_positions_val = torch.tensor(start_positions_val)\n",
      "C:\\Users\\Haider.Abbad\\AppData\\Local\\Temp\\ipykernel_13928\\1678758524.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  end_positions_val = torch.tensor(end_positions_val)\n"
     ]
    }
   ],
   "source": [
    "# Convert validation data into torch tensors\n",
    "input_ids_val = torch.tensor(input_ids_val)\n",
    "attention_masks_val = torch.tensor(attention_masks_val)\n",
    "start_positions_val = torch.tensor(start_positions_val)\n",
    "end_positions_val = torch.tensor(end_positions_val)\n",
    "\n",
    "# Create a DataLoader for validation data\n",
    "val_data = TensorDataset(input_ids_val, attention_masks_val, start_positions_val, end_positions_val)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=16)  # Adjust batch size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.86\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_eval_loss = 0\n",
    "\n",
    "all_start_positions = []\n",
    "all_end_positions = []\n",
    "all_pred_start_positions = []\n",
    "all_pred_end_positions = []\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    # Load batch data to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_attention_mask, b_start_positions, b_end_positions = batch\n",
    "\n",
    "    # Tell the model not to compute gradients\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, start_positions=b_start_positions, end_positions=b_end_positions)\n",
    "        \n",
    "    # Get the predicted start and end token positions\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    pred_start_positions = torch.argmax(start_logits, dim=1)\n",
    "    pred_end_positions = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "    loss = outputs[0]\n",
    "    total_eval_loss += loss.item()\n",
    "\n",
    "    all_start_positions.extend(b_start_positions.tolist())\n",
    "    all_end_positions.extend(b_end_positions.tolist())\n",
    "    all_pred_start_positions.extend(pred_start_positions.tolist())\n",
    "    all_pred_end_positions.extend(pred_end_positions.tolist())\n",
    "\n",
    "avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "print(f\"Validation Loss: {avg_eval_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) Score on Validation Set: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def compute_exact_match(true_starts, true_ends, pred_starts, pred_ends):\n",
    "    return sum([(ts == ps) and (te == pe) for ts, te, ps, pe in zip(true_starts, true_ends, pred_starts, pred_ends)])\n",
    "\n",
    "EM_score = compute_exact_match(all_start_positions, all_end_positions, all_pred_start_positions, all_pred_end_positions)\n",
    "print(f\"Exact Match (EM) Score on Validation Set: {EM_score / len(all_start_positions):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Deployment & Usage (Simplified for Direct Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths where the model and tokenizer were saved\n",
    "model_save_path = './model_save/'\n",
    "tokenizer_save_path = './tokenizer_save/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer_save/tokenizer_config.json',\n",
       " './tokenizer_save/special_tokens_map.json',\n",
       " './tokenizer_save/vocab.txt',\n",
       " './tokenizer_save/added_tokens.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(model_save_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_save_path)\n",
    "\n",
    "# If you have a GPU, let's put the model there for faster computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_building_name_from_span(address, start_idx, end_idx, tokenizer):\n",
    "#     tokens = tokenizer.tokenize(address)\n",
    "#     building_name_tokens = tokens[start_idx: end_idx+1]\n",
    "#     building_name = tokenizer.convert_tokens_to_string(building_name_tokens)\n",
    "#     return building_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_building_names(addresses, model, tokenizer):\n",
    "    # Preprocess the input addresses\n",
    "    input_ids = [tokenizer.encode(a, add_special_tokens=True, max_length=512, pad_to_max_length=True) for a in addresses]\n",
    "    attention_masks = [[1 if token_id > 0 else 0 for token_id in address] for address in input_ids]\n",
    "    \n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    attention_masks = torch.tensor(attention_masks).to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "    start_logits = outputs.start_logits.cpu().numpy()\n",
    "    end_logits = outputs.end_logits.cpu().numpy()\n",
    "\n",
    "    predicted_start = start_logits.argmax(axis=1)\n",
    "    predicted_end = end_logits.argmax(axis=1)\n",
    "\n",
    "    # Post-process to extract building names\n",
    "    building_names = []\n",
    "    for i, address in enumerate(addresses):\n",
    "        tokens = tokenizer.tokenize(address)\n",
    "        building_name_tokens = tokens[predicted_start[i]:predicted_end[i]+1]\n",
    "        building_name = tokenizer.convert_tokens_to_string(building_name_tokens)\n",
    "        building_names.append(building_name)\n",
    "\n",
    "    return building_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['windsor building apt 5a', 'maple complex', 'cedar towers block b']\n"
     ]
    }
   ],
   "source": [
    "# Sample usage:\n",
    "addresses_list = [\n",
    "    \"123 Elm St Windsor Building Apt 5A\",\n",
    "    \"456 Oak Rd Maple Complex Level 2\",\n",
    "    \"789 Pine Ave Cedar Towers Block B\"\n",
    "]\n",
    "\n",
    "predicted_building_names = extract_building_names(addresses_list, model, tokenizer)\n",
    "print(predicted_building_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
